from typing import Type, Literal, Optional
from pydantic import BaseModel, Field
import asyncio
import os

from orbit_agent.skills.base import BaseSkill, SkillConfig
from orbit_agent.skills.vision import VisionSkill, VisionInput, VisionMode
from orbit_agent.skills.desktop import DesktopSkill, DesktopInput, DesktopOutput
from orbit_agent.memory.ui_cache import UICache

class VisualInteractionInput(BaseModel):
    description: str = Field(..., description="Visual description of the element to interact with (e.g. 'the blue submit button', 'the discord server icon with text 03')")
    action: Literal["click", "double_click", "hover", "hover_and_confirm", "verify"] = Field(default="click", description="Action to perform. 'hover_and_confirm' will hover, wait, read tooltip, then click if match.")
    confirm_text: Optional[str] = Field(default=None, description="Text to verify in tooltip/screen if action is 'hover_and_confirm'.")
    
class VisualInteractionOutput(BaseModel):
    success: bool
    data: str = ""
    error: str = ""

class VisualInteractionSkill(BaseSkill):
    def __init__(self, vision_skill: VisionSkill):
        super().__init__()
        self.vision_skill = vision_skill
        self.desktop_skill = DesktopSkill()
        self.cache = UICache()

    @property
    def default_config(self) -> SkillConfig:
        return SkillConfig(
            name="visual_interact",
            description="Locates an element on screen. Uses caching for speed. 'hover_and_confirm' is verification.",
            permissions_required=["vision_analyze", "desktop_control", "desktop_view"]
        )

    @property
    def input_schema(self) -> Type[BaseModel]:
        return VisualInteractionInput

    @property
    def output_schema(self) -> Type[BaseModel]:
        return VisualInteractionOutput

    async def execute(self, inputs: VisualInteractionInput) -> VisualInteractionOutput:
        try:
            # 1. Take Screenshot (Temp)
            fixed_path = os.path.abspath("screenshots/_temp_locate.png")
            os.makedirs(os.path.dirname(fixed_path), exist_ok=True)
            
            # --- CACHE CHECK (The "Memory" Upgrade) ---
            # Use cache if available
            cached_coords = self.cache.get(inputs.description)
            coords = None
            
            if cached_coords:
                print(f"[Memory] Recalled location for '{inputs.description}': {cached_coords}")
                coords = cached_coords
                # If cached, we might skip vision locate, but we still need to verify or act
            
            if not coords:
                view_out = await self.desktop_skill.execute(DesktopInput(action="screenshot"))
                # Note: DesktopSkill saves to 'screenshots/' with timestamp by default, wait, 
                # we need specific path for logic? 
                # DesktopSkill does NOT accept save_path in current impl.
                # Use standard screenshot logic or update DesktopSkill.
                # For now let's use the one generated by DesktopSkill if we can get path, 
                # OR just use mss here? No, stick to skill.
                # Wait, VisualInteraction relies on specific path to pass to Vision.
                # DesktopSkill returns path in data.
                if not view_out.success:
                     return VisualInteractionOutput(success=False, error=f"Screenshot failed: {view_out.error}")
                
                 # Hack: extract path from data "Screenshot saved to ..."
                import re
                try:
                    fixed_path = view_out.data.split("saved to ")[1].strip()
                except:
                     return VisualInteractionOutput(success=False, error="Could not get screenshot path")

                vision_out = await self.vision_skill.execute(VisionInput(
                    image_path=fixed_path,
                    query=inputs.description,
                    mode=VisionMode.LOCATE
                ))
                
                if vision_out.error or not vision_out.coordinates:
                    return VisualInteractionOutput(success=False, error=f"Could not locate '{inputs.description}': {vision_out.error or 'No coordinates found'}")
                
                coords = vision_out.coordinates
                # Save to Memory for next time
                self.cache.set(inputs.description, coords)
            
            x, y = coords
            
            # 3. Action Logic
            if inputs.action == "hover_and_confirm":
                if not inputs.confirm_text:
                    return VisualInteractionOutput(success=False, error="confirm_text required for hover_and_confirm action")
                
                # A. Hover (Uses new human tweening)
                await self.desktop_skill.execute(DesktopInput(action="move", x=x, y=y))
                
                # B. Wait for tooltip
                await asyncio.sleep(1.0)
                
                # C. New Screenshot
                view_out_2 = await self.desktop_skill.execute(DesktopInput(action="screenshot"))
                if not view_out_2.success:
                     return VisualInteractionOutput(success=False, error="Failed to take confirmation screenshot")
                fixed_path = view_out_2.data.split("saved to ")[1].strip()

                # D. Vision Check
                check_query = f"I am hovering over an element. Is the text '{inputs.confirm_text}' OR an icon/symbol representing '{inputs.confirm_text}' (e.g. a Play Triangle, Gear icon, etc.) visible at or near the cursor position? Answer YES or NO."
                vision_check = await self.vision_skill.execute(VisionInput(
                    image_path=fixed_path,
                    query=check_query,
                    mode=VisionMode.DESCRIBE
                ))
                
                if "yes" in vision_check.analysis.lower():
                    # Confirmed -> Click (and reinforce cache)
                    self.cache.set(inputs.description, coords)
                    act_out = await self.desktop_skill.execute(DesktopInput(action="click", x=x, y=y))
                    return VisualInteractionOutput(success=True, data=f"Confirmed '{inputs.confirm_text}' and clicked at {x},{y}")
                else:
                    # Verification Failed!
                    if cached_coords:
                         print(f"[Memory] Cache stale. Initiating Smart Recovery...")
                         self.cache.set(inputs.description, None)
                    
                    # --- SMART VISUAL RECOVERY (The "AGI" Upgrade) ---
                    # 1. Analyze the situation (Chain of Thought)
                    print("Verification failed. Analyzing screen for target...")
                    cot_query = f"I failed to find '{inputs.description}' at the previous location. Look at the full screen. Is the element visible? detailedly describe its visual appearance and position (e.g. 'top right', 'center'). If it's a 'Play' button, look for triangles or 'Join' text."
                    
                    analysis_out = await self.vision_skill.execute(VisionInput(
                        image_path=fixed_path,
                        query=cot_query,
                        mode=VisionMode.DESCRIBE
                    ))
                    
                    # 2. Re-Locate using the specific analysis
                    print(f"Vision Analysis: {analysis_out.analysis[:50]}...")
                    new_locate_query = f"Based on this analysis: '{analysis_out.analysis}', LOCATE the target element '{inputs.description}'."
                    
                    recovery_out = await self.vision_skill.execute(VisionInput(
                        image_path=fixed_path,
                        query=new_locate_query,
                        mode=VisionMode.LOCATE
                    ))
                    
                    if recovery_out.coordinates:
                        xr, yr = recovery_out.coordinates
                        print(f"Smart Recovery found target at {xr},{yr}. Retrying...")
                        
                        # Move & Verify again
                        await self.desktop_skill.execute(DesktopInput(action="move", x=xr, y=yr))
                        await asyncio.sleep(1.0)
                        
                        # Take new screenshot for verification
                        view_out_rec = await self.desktop_skill.execute(DesktopInput(action="screenshot"))
                        fixed_path = view_out_rec.data.split("saved to ")[1].strip()

                        verify_out = await self.vision_skill.execute(VisionInput(
                            image_path=fixed_path,
                            query=check_query,
                            mode=VisionMode.DESCRIBE
                        ))
                        
                        if "yes" in verify_out.analysis.lower():
                            act_out = await self.desktop_skill.execute(DesktopInput(action="click", x=xr, y=yr))
                            # Update Cache with the GOOD coordinates
                            self.cache.set(inputs.description, [xr, yr])
                            return VisualInteractionOutput(success=True, data=f"Recovered and clicked '{inputs.description}' at {xr},{yr}")
                    
                    # If recovery fails...
                    what_is_there_query = "I still can't find it. Briefly describe what IS at the cursor position (text, icon, color) so I can correct my plan."
                    what_is_there = await self.vision_skill.execute(VisionInput(image_path=fixed_path, query=what_is_there_query, mode=VisionMode.DESCRIBE))
                    
                    return VisualInteractionOutput(success=False, error=f"Verification failed after recovery. Expected '{inputs.confirm_text}', but saw: {what_is_there.analysis}")

            # ... verify logic ...
            elif inputs.action == "verify":
                check_query = f"Does the screen clearly contain the text or element '{inputs.description}'? Answer YES or NO."
                vision_check = await self.vision_skill.execute(VisionInput(
                    image_path=fixed_path,
                    query=check_query,
                    mode=VisionMode.DESCRIBE
                ))
                if "yes" in vision_check.analysis.lower():
                     return VisualInteractionOutput(success=True, data=f"Verified presence of '{inputs.description}'")
                else:
                     return VisualInteractionOutput(success=False, error=f"Verification failed: '{inputs.description}' not found.")

            # Standard Actions
            elif inputs.action == "double_click":
                act_out = await self.desktop_skill.execute(DesktopInput(action="click", x=x, y=y))
                await asyncio.sleep(0.1)
                await self.desktop_skill.execute(DesktopInput(action="click", x=x, y=y))
                
            elif inputs.action == "hover":
                act_out = await self.desktop_skill.execute(DesktopInput(action="move", x=x, y=y))
                
            else: # click or default
                 act_out = await self.desktop_skill.execute(DesktopInput(action="click", x=x, y=y))
            
            if not act_out.success:
                return VisualInteractionOutput(success=False, error=f"Action f{inputs.action} failed: {act_out.error}")
                
            return VisualInteractionOutput(success=True, data=f"Located '{inputs.description}' at {x},{y} and performed {inputs.action}")

        except Exception as e:
            return VisualInteractionOutput(success=False, error=str(e))
